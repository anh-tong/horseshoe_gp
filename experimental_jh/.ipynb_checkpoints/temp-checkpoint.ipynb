{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys \n",
    "sys.path.append('../..')\n",
    "\n",
    "import math\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch\n",
    "\n",
    "import gpytorch\n",
    "from gpytorch.means import ZeroMean\n",
    "from gpytorch.kernels import RBFKernel, PeriodicKernel\n",
    "from gpytorch.likelihoods import GaussianLikelihood\n",
    "from gpytorch.mlls import VariationalELBO, PredictiveLogLikelihood\n",
    "\n",
    "from horseshoe_gp.src.structural_sgp import VariationalGP, StructuralSparseGP, \\\n",
    "TrivialSelector, SpikeAndSlabSelector, SpikeAndSlabSelectorV2, HorseshoeSelector\n",
    "from horseshoe_gp.src.mean_field_hs import MeanFieldHorseshoe, VariatioalHorseshoe\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn import svm\n",
    "from sklearn.datasets import load_boston\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from scipy.optimize import minimize, Bounds\n",
    "\n",
    "boston = load_boston()\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    boston['data'],\n",
    "    boston['target'],\n",
    "    test_size=0.2,\n",
    "    random_state=2020)\n",
    "\n",
    "#I am not sure scaling per train and test is appropriate\n",
    "X_train = StandardScaler().fit_transform(X_train)\n",
    "X_test = StandardScaler().fit_transform(X_test)\n",
    "y_train = StandardScaler().fit_transform(np.expand_dims(y_train, axis=1))\n",
    "y_test = StandardScaler().fit_transform(np.expand_dims(y_test, axis=1))\n",
    "\n",
    "#set up kernels\n",
    "n_kernels = 5\n",
    "means = [ZeroMean()] * n_kernels\n",
    "kernels = [RBFKernel()] * n_kernels\n",
    "\n",
    "n_inducing = 50\n",
    "inducing_points = torch.linspace(0, 1, n_inducing)\n",
    "\n",
    "# GP for each kernel\n",
    "gps = []\n",
    "for mean, kernel in zip(means, kernels):\n",
    "    gp = VariationalGP(mean, kernel, inducing_points)\n",
    "    gps.append(gp)\n",
    "    \n",
    "norm = torch.distributions.normal.Normal(0, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "eps-SVM regression on Boston housing dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(param_eps, param_gamma, param_C):\n",
    "    model = svm.SVR(epsilon=param_eps.exp(),\n",
    "                   gamma=param_gamma.exp(),\n",
    "                   C=param_C,\n",
    "                   kernel='rbf')\n",
    "    model.fit(X_train, Y_train)\n",
    "    return mean_squared_error(model.predict(X_test, Y_test), squared = True)\n",
    "\n",
    "num_param = 3\n",
    "\n",
    "bounds = {\n",
    "    'log_eps':[0, -5],\n",
    "    'log_gamma':[0, -5],\n",
    "    'C':[0, 10000]\n",
    "}\n",
    "\n",
    "lb = np.array([0, 0, 0])\n",
    "ub = np.array([-5, -5, 10000])\n",
    "\n",
    "Bounds()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize_UCB(x, bounds, sur_model, kappa, n_warmup = 10000, iteration = 10):\n",
    "    def _UCB(x, kappa):\n",
    "        mean, std = sur_model(x)\n",
    "        return mean + kappa * std\n",
    "    \n",
    "    for iterate in iteration:\n",
    "        locs = torch.empty(n_warmup)._rand()\n",
    "        \n",
    "        res = minimize(lambda x: -self.UCB(locs))\n",
    "        \n",
    "        \n",
    "\n",
    "def optimize_EI(x, bounds, sur_model, kappa, y_max, n_warmup = 10000, iteration = 10):\n",
    "    def _EI(x, kappa):\n",
    "        mean, std = sur_model(x)\n",
    "        a = (mean - ymax - x)\n",
    "        z = a / std\n",
    "        return a * norm.cdf(z) + std * norm.pdf(z)\n",
    "    \n",
    "    for itertate in iteration:\n",
    "        _EI(x, model)\n",
    "        unif\n",
    "    \n",
    "\n",
    "    \n",
    "def optimize_POI(x, bounds, sur_model, kappa, y_max, n_warmup = 10000, iteration = 10):\n",
    "    def _POI(x, y_max):\n",
    "        mean, std = sur_model(x)\n",
    "        z = (mean - y_max - x)/std\n",
    "        return norm.cdf(z)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_trivial():\n",
    "    selector = TrivialSelector(n_kernels)\n",
    "    # main model\n",
    "    model = StructuralSparseGP(gps, selector)\n",
    "    likelihood = GaussianLikelihood()\n",
    "    elbo = VariationalELBO(likelihood, model, num_data=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = svm.SVR(kernel='rbf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVR()"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(boston['data'], boston['target'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.predict()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
